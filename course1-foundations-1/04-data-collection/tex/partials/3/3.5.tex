\subsection{Data as Evidence}

Human beings live in a world filled with uncertainty. Evidence-based decision making has demonstrated neurobiological 
grounding, and evidence science alongside evidence-based decision models are increasingly attracting attention across 
various domains, including policy, welfare, education, robotics, and others.

Having examined measurement, validity, error, and data quality issues, we conclude by addressing the ultimate 
epistemological question: what warrant does data provide for knowledge claims? How do we move from data, which are
 formal representations of phenomena, to justified beliefs about the world? What are the limits of inference from data?

 \subsubsection{From Data to Claims}
The path from data to knowledge claims involves 
interpretation, theoretical framing, and inferential leaps that always outrun what data directly show. 

Moving from data to knowledge claims requires systematic processes of inference that involve multiple levels of transformation
 and interpretation. 
 
 Inference from data involves several fundamental modes: 
(1) deductive inference preserves truth from premises to conclusions but requires premises not established by data alone, 
(2) inductive inference generalizes from observed to unobserved cases, relying on assumptions about uniformity that cannot be 
proven from finite observations, 
(3) abductive inference infers best explanations for observed patterns, but "best" depends on criteria like simplicity
 or coherence that are not given by data. In practice, researchers employ these modes of inference in combination, cycling between
  observation, pattern recognition, and theory construction.The Bayesian perspective provides a formalized framework for understanding this 
  process: evidence is treated as information that updates prior beliefs, with data shifting probability distributions over
   hypotheses according to their likelihood under different theories. This makes the process from data to 
   claims explicit: we always approach data with prior theoretical frameworks, and the role of data is to adjust, refine, 
   or overturn these frameworks. However, Bayesian updating requires specifying priors and likelihood functions, which themselves involve substantive
    commitments that go beyond what data show.However, this process faces \textbf{fundamental inferential gaps} that are irreducible:

\textbf{Data underdetermine theory.} Any finite body of data is logically consistent with multiple theoretical interpretations. Observing correlation
 between variables X and Y is consistent with: X causes Y, Y causes X, both are caused by Z, both reflect W, 
 the correlation is coincidental, or complex combinations. Data constrain but do not uniquely determine theoretical 
 conclusions~\cite{duhem1954aim, quine1951dogmas}.
This underdetermination means moving from data to theory always involves auxiliary assumptions and background theories.
 We interpret data through conceptual frameworks that specify what patterns are meaningful, what relationships are plausible, what mechanisms
  are operative. Different frameworks generate different interpretations from the same data.
For example, economic data on trade and growth are consistent with free trade theory, structuralist dependency theory, and
 various intermediate positions. Which interpretation seems most plausible depends on prior theoretical commitments about markets,
  power, and development. The data themselves do not resolve these debates.
The Bayesian perspective formalizes this relationship by treating evidence as information that updates prior beliefs. Data shift probability
 distributions over hypotheses according to their likelihood under different theories. This makes the dependence on priors explicit
  while providing a coherent framework for learning from evidence. However, Bayesian updating requires specifying priors and
   likelihood functions, which themselves involve substantive commitments that go beyond what data show.

\textbf{Theories involve claims beyond data.} Theoretical concepts like "social capital," "state capacity," or "democratic consolidation" 
transcend any particular set of observations. They make claims about unobservables, counterfactuals, and modal properties
 (what would happen under different conditions). Data provide evidence about observables in actual situations. The
  gap between these is bridged by inference, not deduction.
Causal claims are particularly ambitious. Asserting that X causes Y claims not just that they correlate but that
 intervening on X would change Y, that this relationship is stable across contexts, that it operates through specific mechanisms. Data
  rarely directly demonstrate these stronger claims. Inference from data always involves one of several modes: 
  deductive inference preserves truth from premises to conclusions but requires premises not established by data alone; inductive inference
   generalizes from observed to unobserved cases, relying on assumptions about uniformity that cannot be proven from finite 
   observations; abductive inference infers best explanations for observed patterns, but "best" depends on criteria like
    simplicity or coherence that are not given by data.

\textbf{Observation is theory-laden.} As discussed earlier, what we observe is structured by theoretical frameworks. This creates hermeneutic circularity:
 we interpret data through theories, and theories are evaluated against data. There is no theory-neutral
  observation that could arbitrate between competing frameworks~\cite{hanson1958patterns}.
This does not make empirical research futile or arbitrary. It means empirical inquiry is always dialogical, 
moving between theory and evidence, refining both through their interaction. Theories face empirical accountability even if they are not strictly
 proven or falsified by data.

 Recognition of the inferential gaps between data and claims has direct implications for how we should collect and preserve data. 
 If data underdetermine theory, if theories make claims beyond observables, and if observation is theory-laden, 
 then responsible data collection must be designed to support multiple possible inferential paths and enable scrutiny of the assumptions linking
  data to claims.

\textbf{Transparency and documentation.} Because the path from observation to data involves choices about 
measurement, coding, and categorization, these choices must be explicitly documented. Data collection protocols should record not just final
 datasets but also the operational decisions that produced them: how concepts were operationalized, how categories were
  defined, how ambiguous cases were handled, what information was excluded and why. This documentation allows others to
   assess whether different theoretical frameworks would interpret the data differently, and enables evaluation of how measurement choices
    affect conclusions.

    \textbf{Preserving granularity and context.} Since theoretical frameworks evolve and new questions emerge, data should
 be collected and preserved at the finest feasible level of granularity rather than being immediately aggregated or simplified. 
 Raw or minimally processed data retain more information than summary statistics or pre-coded categories. Contextual
  information about data collection circumstances, even when not immediately relevant to current research questions, may
   prove essential for future reanalysis or alternative interpretations. What seems like unnecessary detail from one theoretical perspective may be
    crucial evidence from another.

    \textbf{Multiple operationalizations.} Given that observation is theory-laden and concepts can be
     measured in various ways, collecting multiple indicators of theoretical constructs strengthens inference. Rather than committing
      to a single operationalization that embeds particular theoretical assumptions, measuring concepts through diverse approaches
       allows assessment of whether conclusions depend on specific measurement choices. Triangulation across different 
       operationalizations helps distinguish robust findings from measurement artifacts.

       \textbf{Auxiliary information for assumption testing.} Because inference from data requires auxiliary
        assumptions about selection processes, missing data mechanisms, measurement error structures, 
        and causal identification, data collection should anticipate the need to test these assumptions. This means collecting
         information beyond the focal variables of immediate interest: data on sampling processes, non-response patterns, measurement 
         validation, and potential confounders. Such auxiliary data may not be central to primary research questions but becomes essential 
         when evaluating the plausibility of inferential assumptions.

\textbf{Enabling replication and reanalysis.} Since knowledge accumulates through multiple studies and alternative analyses rather than single
 definitive tests, data should be collected and preserved in ways that enable independent replication and reanalysis. This 
 requires not just sharing final datasets but also preserving materials that allow others to reconstruct the data generation process: 
 survey instruments, coding schemes, administrative protocols, sampling frames. Open data practices, where feasible and 
 ethical, support the collective scrutiny essential to scientific inference.

 \textbf{Structured uncertainty quantification.} Data collection should build in mechanisms for quantifying the uncertainties 
 that inevitably propagate through inferential chains. This includes: collecting repeated measurements to assess measurement reliability,
  recording information about data quality variation across observations, documenting known gaps or limitations in coverage, and
   preserving information about the precision of recording instruments or procedures. Rather than treating data as fixed facts, 
   collection practices should acknowledge and document their provisional and uncertain character.

\textbf{Ethical data practices as epistemic requirements.} The ethical imperative to minimize harm and respect autonomy in data collection
 also serves epistemic functions. Informed consent processes that honestly describe research purposes and data uses reduce 
 deception and strategic response. Privacy protections that limit data collection to genuinely necessary information focus attention 
 on theoretically motivated measurement rather than opportunistic data gathering. Participatory approaches that involve research subjects in
  defining relevant questions and appropriate measurements can surface theory-laden assumptions that researchers might otherwise overlook.
In summary, the fundamental gaps between data and claims do not counsel despair about empirical research but rather demand reflexive, 
rigorous, and transparent data collection practices. We cannot eliminate inferential uncertainty, but we can design data collection
 to make our inferential assumptions explicit, enable critical evaluation of those assumptions, and support the cumulative, collective
  process through which reliable knowledge emerges from uncertain evidence. Good data collection is not simply technical execution but
   epistemologically informed practice that anticipates how data will be used as evidence for knowledge claims.


\subsubsection{Strength of Evidence}

Not all evidence is equal. Some data provide stronger warrant for claims 
than others. What makes evidence strong?

\textbf{Replicability} increases confidence in findings. If a relationship 
appears consistently across independent studies, multiple datasets, and 
different researchers, this suggests robustness rather than chance or 
investigator-specific artifacts. The replication crisis in social science 
has revealed that many published findings do not replicate, undermining 
confidence in those results~\cite{open2015estimating}.

However, replication is complex in social science. Exact replication is 
often impossible because social contexts change. Conceptual replication—
finding the same theoretical relationship in different empirical contexts—
is valuable but involves judgment about whether contexts are sufficiently 
similar. Failed replications could indicate original findings were spurious 
or that scope conditions differ between original and replication contexts.

\textbf{Coherence across methods} increases confidence when findings converge 
despite using different approaches with non-overlapping weaknesses. If 
experiments, observational studies, and qualitative research agree, this 
triangulation suggests findings are not methodological artifacts. 

Campbell and Fiske's~\cite{campbell1959convergent} logic applies at the 
study level: if different methods with different biases agree, the shared 
conclusion is more credible than any single method provides.

However, method triangulation requires that methods actually address the same 
question. Different methods may capture different aspects of phenomena or 
apply to different scope conditions. Apparent disagreement might reflect 
complementary rather than contradictory findings.

\textbf{Theoretical integration} strengthens evidence when findings fit 
coherently into broader theoretical frameworks. Isolated findings are less 
credible than those that make sense given other knowledge. A finding that 
contradicts well-established relationships requires stronger evidence than 
one that extends or specifies existing understanding.

This connects to Whewell's~\cite{whewell1840philosophy} \textit{consilience 
of inductions}—when evidence from independent domains supports the same 
theoretical conclusion, confidence increases dramatically. Darwinian evolution 
is credible partly because paleontology, comparative anatomy, biogeography, 
and genetics independently support it.

However, theoretical integration can also be conservative, making genuinely 
surprising findings difficult to accept even when evidentially warranted. 
Balance is required between healthy skepticism of anomalies and openness to 
genuine theoretical revision.

\textbf{Quantitative precision} can be misleading. Statistical significance, 
large sample sizes, and narrow confidence intervals do not necessarily indicate 
strong evidence if underlying data quality is poor or key assumptions are 
violated. Precision without accuracy is not valuable.

Conversely, some evidence is qualitative but strong—detailed case studies 
revealing mechanisms, ethnographic observations of processes, archival 
documentation of historical sequences. These do not easily reduce to 
quantitative metrics but can provide powerful evidence.

\textbf{Effect sizes and practical significance} matter beyond statistical 
significance. A statistically significant but tiny effect may be real but 
unimportant. A large effect that barely misses significance due to small 
samples may be substantively important. Interpreting evidence requires 
attention to magnitude, not just statistical tests~\cite{ziliak2008cult}.

\textbf{Prospective prediction} provides stronger evidence than retrospective 
fitting. If a theory predicts novel patterns that are subsequently confirmed, 
this is more impressive than fitting theory to already-known patterns. 
Accommodation of known facts is weak evidence; successful novel prediction 
is strong evidence.

This asymmetry reflects the problem of overfitting and data mining. Given 
enough flexibility, any dataset can be fitted, but only theories capturing 
genuine patterns successfully predict new data.

\subsubsection{Causal versus Correlational Evidence}

A crucial distinction in evaluating evidence is between causal and correlational 
claims. Correlation—systematic covariation between variables—is relatively 
easy to establish. Causation—claims that changes in X produce changes in Y—
requires stronger evidence and additional assumptions.

\textbf{Hierarchy of evidence for causal inference:} Methodologists often 
rank research designs by their capacity to support causal claims:

\textit{Randomized controlled trials (RCTs)} are considered the gold standard 
because randomization ensures treatment assignment is independent of potential 
outcomes in expectation. Any difference between treatment and control groups 
can be attributed to treatment rather than selection~\cite{fisher1935design}.

However, even RCTs face challenges: compliance may be imperfect, attrition 
may be differential, spillovers may violate non-interference assumptions, 
Hawthorne effects may mean treatment in research settings differs from 
treatment in practice. And RCTs often examine narrow, well-defined treatments 
in artificial settings, limiting external validity and ecological validity.

\textit{Quasi-experimental designs} approximate randomization through natural 
experiments, instrumental variables, regression discontinuity, or 
difference-in-differences. These identify causal effects under weaker 
assumptions than simple regression but stronger assumptions than true 
randomization~\cite{angrist2008mostly}.

Each quasi-experimental approach has identifying assumptions that cannot be 
fully tested: instruments must satisfy exclusion restrictions, discontinuities 
must not coincide with other changes, parallel trends must hold in 
difference-in-differences. When assumptions are plausible, quasi-experiments 
provide credible causal evidence. When implausible, they do not.

\textit{Observational studies with extensive controls} attempt to adjust for 
confounding through measuring and controlling for covariates. This succeeds 
if all confounders are observed (conditional independence) and functional 
forms are correct~\cite{rosenbaum1983central}.

However, this is a very strong assumption. Unobserved confounding is always 
possible. Even with rich data, we cannot be certain all relevant variables 
are measured. Sensitivity analyses can examine how much unobserved confounding 
would alter conclusions.

\textit{Simple correlational analyses} establish association but cannot 
distinguish causation from confounding or reverse causation without additional 
assumptions. Yet correlation is not worthless—it can be highly informative 
about co-occurrence even without causal identification.

This hierarchy is useful but should not be treated rigidly. Context matters. 
A well-designed observational study with excellent measurement and controls 
may provide stronger evidence than a poorly implemented RCT. Qualitative 
evidence of mechanisms can strengthen causal claims even without 
randomization.

\textbf{Mechanisms and process tracing} provide complementary evidence for 
causation. Even with randomized treatment, understanding \textit{how} 
treatment affects outcomes—the mechanisms or pathways—increases confidence 
in causal claims and informs external validity~\cite{hedstrom2010causal}.

If we observe X and Y correlate, and we can document intermediate steps 
connecting them, this mechanistic evidence supports causation beyond mere 
correlation. Process tracing in case studies examines whether expected 
mechanisms actually operate in practice~\cite{beach2013doing}.

For example, if we claim democracy prevents conflict, mechanistic accounts 
specify pathways: transparency constraints on leaders, institutional checks 
on unilateral action, norms of peaceful dispute resolution. Finding these 
mechanisms actually operating in cases strengthens causal inference beyond 
what cross-national correlations alone provide.

\textbf{Counterfactual reasoning} underlies all causal inference. Causal 
claims implicitly invoke counterfactuals: what would have happened without 
treatment? This counterfactual is never directly observed—we observe either 
treatment or control for any given unit, not both~\cite{rubin1974estimating}.

Research designs approximate counterfactuals through comparison groups that 
are as similar as possible to treated units. The credibility of causal 
inference depends on how plausible the comparison is as a counterfactual. 
Randomization makes this credible by design. Observational studies must argue 
for plausibility through design and controls.

\subsubsection{Evidence and Decision-Making}

Finally, we address how data as evidence relates to practical decisions. 
Evidence-based policy and practice have become influential paradigms, but 
translating research evidence to action involves additional considerations 
beyond scientific inference.

\textbf{Evidence thresholds depend on decision context.} Different decisions 
require different levels of certainty. Medical interventions require strong 
evidence of safety and efficacy because risks are high. Exploratory policies 
might proceed with weaker evidence if costs are low and learning value is 
high. Emergency responses must act on limited evidence because delay has costs.

This means "sufficient evidence" is not a fixed threshold but depends on the 
stakes of being wrong, costs of gathering more evidence, and value of action 
versus inaction. Science aims for knowledge; policy aims for good outcomes. 
These align but are not identical~\cite{cartwright2012evidence}.

\textbf{Multiple value dimensions matter.} Decisions involve not just 
empirical evidence about means-ends relationships but also values about what 
ends to pursue, how to trade off competing goods, and who bears costs and 
benefits. Evidence can inform but cannot resolve fundamentally normative 
questions.

For example, evidence might show that certain education policies improve test 
scores. Whether to implement them depends additionally on whether test scores 
are the right metric, whether improvements are worth costs, whether 
distributional effects are acceptable, and how education fits with broader 
social aims. These are not purely empirical questions.

\textbf{Epistemic humility} is particularly important when evidence informs 
decisions. Research always involves uncertainty—measurement error, causal 
ambiguity, limited external validity, theoretical underdetermination. 
Decision-makers should not expect certainty from research but rather 
probabilistic information that reduces uncertainty.

This suggests decision frameworks that explicitly account for uncertainty: 
scenario planning that considers multiple possibilities, adaptive policies 
that adjust as more is learned, robust strategies that perform reasonably 
under various conditions rather than optimally under one specific assumption.

\textbf{Precautionary reasoning} becomes important when potential harms are 
large and evidence is incomplete. Waiting for definitive evidence may itself 
be risky if harmful processes are operating. Climate change policy debates 
exemplify this tension: how much evidence is sufficient given catastrophic 
downside risks versus costs of aggressive action?

Precautionary reasoning does not mean ignoring evidence but rather 
acknowledging asymmetric risks. When potential losses are large and possibly 
irreversible, even uncertain evidence may warrant action. This is legitimate 
practical reasoning even if it differs from scientific standards.

\textbf{Evidence synthesis and systematic review} help translate research 
to decision-making. Rather than relying on single studies, systematic reviews 
synthesize bodies of evidence, assess quality, and provide overall 
assessments~\cite{petticrew2006systematic}. Meta-analysis quantitatively 
combines effect estimates, increasing precision and examining heterogeneity.

However, evidence synthesis faces challenges: publication bias favors 
significant findings, study quality varies, heterogeneity may reflect 
meaningful contextual differences rather than noise, and inclusion criteria 
involve judgment about relevance. Synthesis improves evidence base but does 
not eliminate uncertainty or judgment.

\textbf{Stakeholder participation} recognizes that those affected by decisions 
possess practical knowledge relevant to evidence interpretation and 
implementation. Researchers understand methods and findings; practitioners 
understand contexts and feasibility; affected communities understand lived 
experience and values. Evidence-informed decision-making requires integrating 
these different forms of knowledge~\cite{nutley2007using}.

This points toward \textit{co-production} models where research is conducted 
with rather than on communities, and evidence is jointly interpreted with 
stakeholders rather than delivered from researchers to passive recipients.

In conclusion, data provide evidence, structured information that reduces uncertainty about the world. But 
evidence always involves interpretation, operates within theoretical frameworks, contains uncertainty, and must
 be integrated with values for decision-making. Rigorous quantitative social science requires attending carefully to all these dimensions:
  not just collecting data but understanding what warrant data provide for what kinds of claims under what conditions.
The measurement, validity, and data quality issues examined throughout this chapter are not merely technical obstacles to be
 overcome through better methods. They reflect fundamental features of studying complex, meaning-making, reflexive social systems. 
 Perfect measurement is impossible. Validity is contestable. Error is inevitable. Data are always incomplete and partial. Recognizing
  these limitations is the starting point for rigorous social science, not an admission of failure. It shapes how we design research, 
  interpret findings, and communicate results. It reminds us that quantitative social science, like all human knowledge, is a 
  collective, fallible, ongoing enterprise of making sense of a complex world.